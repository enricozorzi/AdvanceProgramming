{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Search for new activation functions**\n",
    "---\n",
    "We generate a random population of activation functions, each represented as a mathematical formula.\n",
    "* **Unary functions**: x, −x, |x|, x2, x3,√x, βx, x + β, log(|x| + ε), exp(x) sin(x), cos(x), sinh(x), cosh(x), tanh(x), sinh−1(x), tan−1(x), <br> sinc(x), max(x, 0), min(x, 0), σ(x),log(1 + exp(x)), exp(−x2), erf(x)\n",
    "\n",
    "\n",
    "* **Binary functions**: x1 + x2, x1 · x2, x1 − x2,x1/(x2+ε), max(x1, x2), min(x1, x2), σ(x1) · x2, exp(−β(x1 − x2)^2), exp(−β|x1 − x2|), βx1 + (1 − β)x2\n",
    "<figure align=\"center\">\n",
    "    <img  width=\"800\" height=\"250\" src=\"https://miro.medium.com/v2/resize:fit:1400/1*ccoqEVXPa0lf01ibFPy9tA.png\">\n",
    "    <figcaption>The top novel activation functions found by the searches. Separated into two diagrams for visual clarity. Best viewed in color.</figcaption>\n",
    "</figure>\n",
    "To test the robustness of the top performing novel activation functions to different architectures, we run additional experiments using the preactivation ResNet-164 (RN), Wide ResNet 28-10 (WRN), and DenseNet 100-12 (DN) models.\n",
    "<figure align=\"center\">\n",
    "    <img  width=\"800\" height=\"250\" src=\"https://miro.medium.com/max/1400/1*NkOt2sKckxOsXwWS3-KTqw.png\">\n",
    "</figure>\n",
    "The results are shown in Tables 1 and 2. Two of the discovered activation functions, x·σ(βx)\n",
    "and max(x, σ(x)), consistently match or outperform ReLU on all three models.<br>\n",
    "We focus on empirically evaluating the activation\n",
    "function \n",
    "\n",
    "$ f(x)=x*σ(βx) $\n",
    "\n",
    "and we call **Swish**.\n",
    "\n",
    "---\n",
    "\n",
    "## **The swish activation function**\n",
    "---\n",
    "The swish activation function was introduced in 2017 as an alternative to the commonly used ReLU activation function. <br>\n",
    "Like the ReLU function, the swish function is non-linear.\n",
    "\n",
    "The mathematical definition of the swish function:\n",
    "$$\n",
    " swish(x) = x * sigmoid(βx) \n",
    "$$\n",
    "where β is either a constant or a trainable parameter and sigmoid(x) is the standard sigmoid function, defined as:\n",
    "\n",
    "$$ sigmoid(z) = \\frac{1}{1 + exp(-z)} $$\n",
    "\n",
    "<br>\n",
    "\n",
    "<figure align=\"center\">\n",
    "  <img width=\"460\" height=\"300\" src=\"https://media.geeksforgeeks.org/wp-content/uploads/20200516225910/swish.jpeg\">\n",
    "  <img width=\"460\" height=\"300\" src=\"https://media.arxiv-vanity.com/render-output/6513421/x4.png\">\n",
    "  <figcaption>If β = 1, Swish is equivalent to the Sigmoid-weighted Linear Unit (SiL) of Elfwinget al.<br>\n",
    "If β = 0, Swish becomes the scaled linear function f(x) = x2. <br>\n",
    "As β → ∞, the sigmoid component approaches a 0-1 function, so Swish becomes like the ReLU function.<br></figcaption>\n",
    "</figure>\n",
    "<br>\n",
    "\n",
    "Benefit of the swish function: \n",
    "*   The most striking difference between Swish and ReLU is the non-monotonic “bump” of Swish when x < 0, the shape of this bump can be controlled by changing the β parameter.\n",
    "\n",
    "Limitation of the swish function:\n",
    " *   Is not computationally efficient as the ReLU function, because it requires a sigmoid computation for each input value. This can make it slower to train and evaluate neural network models that use the swish function.\n",
    "\n",
    "---\n",
    "\n",
    "## **Experiment Compare Relu with Swhis** \n",
    "---\n",
    "### Simple classification problem\n",
    "In this problem, we classifying images of handwritten digits (0-9) using the MNIST dataset, the goal of this problem is to develop a model that can accurately classify images of handwritten digits (0-9) using the MNIST dataset.\n",
    "\n",
    "#### **Relu** activation function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Network (ReLU)\n",
      "epoch_0 ->[====================] 100%\n",
      "Accuracy of the network: 92.68 %\n",
      "TimeCost for train:  0:00:59.245795\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "net = SimpleNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "mnist_train = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=64, shuffle=True)\n",
    "print(\"Train Network (ReLU)\")\n",
    "\n",
    "# Train the network\n",
    "tstart = datetime.now()\n",
    "for epoch in range(1):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        j = (i + 1) / len(train_loader)\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write(\"epoch_%d ->[%-20s] %d%%\" % (epoch,'='*int(20*j), 100*j))\n",
    "        sys.stdout.flush()\n",
    "tend = datetime.now()\n",
    "\n",
    "# Evaluate the network on the test dataset\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('\\nAccuracy of the network: {} %'.format(100 * correct / total))\n",
    "print(\"TimeCost for train: \",tend-tstart)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Swish** (silu) activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Network (Swish)\n",
      "epoch_0 ->[====================] 100%\n",
      "Accuracy of the network: 94.77 %\n",
      "TimeCost for train:  0:01:12.321490\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = torch.nn.functional.silu(self.fc1(x))\n",
    "        x = torch.nn.functional.silu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "net = SimpleNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "mnist_train = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=64, shuffle=True)\n",
    "print(\"Train Network (Swish)\")\n",
    "# Train the network\n",
    "tstart = datetime.now()\n",
    "for epoch in range(1):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        j = (i + 1) / len(train_loader)\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write(\"epoch_%d ->[%-20s] %d%%\" % (epoch,'='*int(20*j), 100*j))\n",
    "        sys.stdout.flush()\n",
    "tend = datetime.now()\n",
    "       \n",
    "\n",
    "# Evaluate the network on the test dataset\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('\\nAccuracy of the network: {} %'.format(100 * correct / total))\n",
    "print(\"TimeCost for train: \",tend-tstart)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0bf133bb5c8d596d0b3316748cc22430568bd4b66fc3c568d3c8f985365b2cda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
